{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c25f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemma\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/autodl-tmp/gemma-3-1b-it \\\n",
    "    --dataset seek \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template gemma \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/gemma/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 2 \\\n",
    "    --save_steps 10 \\\n",
    "    --eval_steps 5 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 6.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea5c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/gemma-3-1b-it \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/gemma/lora/sft  \\\n",
    "    --eval_dataset seek_salary \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template gemma \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/gemma/lora/sft/predict_salary \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fff8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/gemma-3-1b-it \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/gemma/lora/sft  \\\n",
    "    --eval_dataset seek_seniority \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template gemma \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/gemma/lora/sft/predict_seniority \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/gemma-3-1b-it \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/gemma/lora/sft  \\\n",
    "    --eval_dataset seek_work \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template gemma \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/gemma/lora/sft/predict_work \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/autodl-tmp/qwen-2.5-1.5b-instruct \\\n",
    "    --dataset seek \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 2 \\\n",
    "    --save_steps 10 \\\n",
    "    --eval_steps 5 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 6.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e627a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/qwen-2.5-1.5b-instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/qwen/lora/sft  \\\n",
    "    --eval_dataset seek_salary \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen/lora/sft/predict_salary1 \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4692a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/qwen-2.5-1.5b-instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/qwen/lora/sft  \\\n",
    "    --eval_dataset seek_seniority \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen/lora/sft/predict_seniority1 \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f46143",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/qwen-2.5-1.5b-instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/qwen/lora/sft  \\\n",
    "    --eval_dataset seek_work \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen/lora/sft/predict_work1 \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/autodl-tmp/phi-3-mini-instruct \\\n",
    "    --dataset seek \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template phi \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/phi/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 2 \\\n",
    "    --save_steps 10 \\\n",
    "    --eval_steps 5 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 6.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a291059",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/phi-3-mini-instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/phi/lora/sft  \\\n",
    "    --eval_dataset seek_work \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template phi \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/phi/lora/sft/predict_work \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b36bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/phi-3-mini-instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/phi/lora/sft  \\\n",
    "    --eval_dataset seek_salary \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template phi \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/phi/lora/sft/predict_salary \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/phi-3-mini-instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/phi/lora/sft  \\\n",
    "    --eval_dataset seek_seniority \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template phi \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/phi/lora/sft/predict_seniority \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/autodl-tmp/llama-3.2-1b-instruct \\\n",
    "    --dataset seek \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 2 \\\n",
    "    --save_steps 40 \\\n",
    "    --eval_steps 20 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 6.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef66a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path  /root/autodl-tmp/llama-3.2-1b-instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/llama/lora/sft  \\\n",
    "    --eval_dataset seek_seniority \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama/lora/sft/predict_seniority \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/llama-3.2-1b-instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/llama/lora/sft  \\\n",
    "    --eval_dataset seek_salary \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama/lora/sft/predict_salary \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/autodl-tmp/llama-3.2-1b-instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/llama/lora/sft  \\\n",
    "    --eval_dataset seek_work \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama/lora/sft/predict_work \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama3\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/autodl-tmp/Llama-3.2-3B-Instruct \\\n",
    "    --dataset seek \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama3/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 2 \\\n",
    "    --save_steps 40 \\\n",
    "    --eval_steps 20 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f39556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama31\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/autodl-tmp/Llama-3.2-3B-Instruct \\\n",
    "    --dataset seek_train_salary \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama31/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 2 \\\n",
    "    --save_steps 40 \\\n",
    "    --eval_steps 20 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen7\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/Qwen2.5-7B-Instruct \\\n",
    "    --dataset seek \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen7/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 50 \\\n",
    "    --save_steps 40 \\\n",
    "    --eval_steps 40 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c59e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/autodl-tmp/Llama-3.2-3B-Instruct \\\n",
    "    --dataset seek_train_salary \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama31/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 2 \\\n",
    "    --save_steps 40 \\\n",
    "    --eval_steps 20 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/autodl-tmp/Llama-3.2-3B-Instruct \\\n",
    "    --dataset seek \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama3b/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 2 \\\n",
    "    --save_steps 40 \\\n",
    "    --eval_steps 20 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79469264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen71\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /root/Qwen2.5-7B-Instruct \\\n",
    "    --dataset seek_train_salary \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen7b/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 256 \\\n",
    "    --preprocessing_num_workers 4 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --warmup_steps 50 \\\n",
    "    --save_steps 20 \\\n",
    "    --eval_steps 20 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --metric_for_best_model eval_loss \\\n",
    "    --greater_is_better False \\\n",
    "    --learning_rate 1e-6 \\\n",
    "    --num_train_epochs 1.0 \\\n",
    "    --val_size 0.2 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e63dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/Qwen2.5-7B-Instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/qwen7/lora/sft  \\\n",
    "    --eval_dataset seek_work \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen7/lora/sft/predict_work \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 256 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe341c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/Qwen2.5-7B-Instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/qwen7/lora/sft  \\\n",
    "    --eval_dataset seek_salary \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen7/lora/sft/predict_salary \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/Qwen2.5-7B-Instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/qwen7salary/lora/sft  \\\n",
    "    --eval_dataset seek_seniority \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen7salary/lora/sft/predict_seniority \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /root/Qwen2.5-7B-Instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/qwen7/lora/sft  \\\n",
    "    --eval_dataset seek_seniority \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/qwen7/lora/sft/predict_seniority \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 128 \\\n",
    "    --preprocessing_num_workers 12 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6110df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path  /root/autodl-tmp/Llama-3.2-3B-Instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/llama3b/lora/sft  \\\n",
    "    --eval_dataset seek_work \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama3b/lora/sft/predict_work \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 256 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 10 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path  /root/autodl-tmp/Llama-3.2-3B-Instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/llama3b/lora/sft  \\\n",
    "    --eval_dataset seek_salary \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama3b/lora/sft/predict_salary \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 256 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 10 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca02253",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path  /root/autodl-tmp/Llama-3.2-3B-Instruct \\\n",
    "    --adapter_name_or_path /root/autodl-tmp/sft/llama3b/lora/sft  \\\n",
    "    --eval_dataset seek_seniority \\\n",
    "    --dataset_dir /root/LLaMA-Factory/data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir /root/autodl-tmp/sft/llama3b/lora/sft/predict_seniority \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 256 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 10 \\\n",
    "    --predict_with_generate"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
