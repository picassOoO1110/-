{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7070ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8676dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from local\n",
    "model_name = \"/root/autodl-tmp/gemma-3-1b-it\"\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map=\"auto\",          \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8b0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(raw_html):\n",
    "    if pd.isna(raw_html):\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    return soup.get_text(separator=\" \", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb49c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/root/autodl-tmp/data/seniority_labelled_development_set.csv\")  \n",
    "\n",
    "def row_to_text(row, include_label=True):\n",
    "    base = (\n",
    "        f\"Job Title: {row['job_title']}\\n\"\n",
    "        f\"Summary: {row['job_summary']}\\n\"\n",
    "        f\"Details: {row['job_ad_details']}\\n\"\n",
    "        f\"classification_name: {row['classification_name']}\\n\"\n",
    "        f\"subclassification_name: {row['subclassification_name']}\\n\"\n",
    "    )\n",
    "    return base + (f\"\\nLabel: y_true = {row['y_true']}\" if include_label else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e39b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use first 10 data in train set as few-shot\n",
    "shots = \"\\n\\n\".join([f\"example{i+1}：\\n{row_to_text(train_df.iloc[i])}\" for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4616d54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100,save result to：gemma/gemma-1b-seniority_predictions_zeroshot.csv\n",
      "200,save result to：gemma/gemma-1b-seniority_predictions_zeroshot.csv\n",
      "300,save result to：gemma/gemma-1b-seniority_predictions_zeroshot.csv\n",
      "400,save result to：gemma/gemma-1b-seniority_predictions_zeroshot.csv\n",
      "500,save result to：gemma/gemma-1b-seniority_predictions_zeroshot.csv\n",
      "600,save result to：gemma/gemma-1b-seniority_predictions_zeroshot.csv\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/root/autodl-tmp/data/seniority_labelled_test_set.csv\")\n",
    "\n",
    "\n",
    "test_df[\"predicted\"] = \"\"\n",
    "test_df[\"true_label\"] = test_df[\"y_true\"].astype(str)\n",
    "results = []\n",
    "correct=0\n",
    "\n",
    "for i, row in test_df.iterrows():\n",
    "    query = f\"query：\\n{row_to_text(row, include_label=False)}\\nyour_predict：y_true =\"\n",
    "\n",
    "    user_prompt = (\n",
    "        \"You are a classification assistant. Below are some job descriptions along with their corresponding seniorities (y_true)：\\n\\n\"\n",
    "        \"Please only give me answer, no more other things,like experienced, intermediate, senior, entry level, assistant, lead, head, junior and so on.)\"\n",
    "        # f\"{shots}\\n\\n{query}\"\n",
    "        f\"{query}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=80,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    gen_ids = outputs[0][model_inputs[\"input_ids\"].shape[-1]:]\n",
    "    prediction = tokenizer.decode(gen_ids, skip_special_tokens=True).strip().split(\"\\n\")[0]\n",
    "\n",
    "    results.append({\n",
    "        \"index\": i,\n",
    "        \"predicted\": prediction,\n",
    "        \"true_label\": str(row[\"y_true\"])\n",
    "    })\n",
    "    if prediction == str(row[\"y_true\"]):\n",
    "        correct+=1\n",
    "    # save\n",
    "    if (i + 1) % 100 == 0:\n",
    "        checkpoint_df = pd.DataFrame(results)\n",
    "        filename = f\"gemma/gemma-1b-seniority_predictions_zeroshot.csv\"\n",
    "        checkpoint_df.to_csv(filename, index=False)\n",
    "        print(f\"{i+1},save result to：{filename}\")\n",
    "    number=i\n",
    "    # if i ==220:\n",
    "    #     break\n",
    "\n",
    "\n",
    "filename = f\"gemma/gemma-1b-seniority_predictions_zeroshot.csv\"\n",
    "final_df = pd.DataFrame(results)\n",
    "final_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07cd4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ffe43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37101be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=correct/number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce138dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy：0.58%\n",
      "save result to：gemma/gemma-1b-seniority_predictions_zeroshot.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"accuracy：{accuracy:.2%}\")\n",
    "print(f\"save result to：{filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
